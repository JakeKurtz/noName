<!DOCTYPE html>
<html lang="en">
<head>
    <title>noName... </title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="prism.css" data-noprefix />
    <meta name="viewport" content="width=device-width, height=device-height, initial-scale=1.0, minimum-scale=1, maximum-scale=1.0, user-scalable=no">
    <script src="prism.js"></script>
</head>
<body>
    <div class="container">
        <h1 align="center">A Weblog</h1>
        <hr>
        <header>
            <div class="navbar">
                <a href="index.html">HOME</a>
                <div class="subnav">
                    <button class="subnav_btn">PHOTOGRAPHY</button>
                    <div class="subnav_content">
                        <a href="gallery.html">NATURE</a>
                        <a href="gallery.html">PEOPLE</a>
                        <a href="gallery.html">ASTROPHOTOGRAPHY</a>
                        <a href="gallery.html">LANDSCAPE</a>
                    </div>
                </div>
                <a href="blog.html">WEBLOG</a>
            </div>
        </header>
        <div class="article">
            <h1>Astrophotography: Trials and Tribulations</h1>
            <h4 class="date">July 13, 2020</h4>
            <hr>

            <p>
                Okay, so last week I began my journey into astrophotography. Let's just say it's been educational.
                <br><br>
                I'll start by showing off my setup.
                <br><br>
                A lens support bracket that I built from some stuff I picked up from the hardware store. Planning to add some rubber tape, support for the lens, a velcro strap, and a little knob to screw in the camera easier.
            </p>
            <img class="demoImage" width=400 src="https://i.imgur.com/XeF1hkc.jpg">
            <p>
                Using a Sony a6000 with an old school Olympus 85-300mm lens that I order off of ebay for like 50 bucks.
            </p>
            <div class="container_center">
                <img class="img_row" width=400 src="https://i.imgur.com/AzehBGb.jpg"><img class="img_row" width=400 src="https://i.imgur.com/RunwYrp.jpg">
            </div>
            <p>
                Since my balcony has a pretty good view of the south-southeastern sky, I started there to look for some cool deep space objects.
                <br><br>
                I decided to start with the blue horsehead nebula, since it has a magnitude of 4.03 and is quite large.
                <br><br>
                Once I worked out the kinks with the auto shutter/continuous shooting and getting enough storage, I immediately ran into issues. Since the nebula is kinda close to the horizon the lens flare from the street lights was giving me some serious issues and limited the amount of time I could shoot for. This is what 5 minutes of integration looks like,
            </p>
            <img class="demoImage" width=1000 src="https://i.imgur.com/goBIiGj.jpg">
            <p>
                and even then you can still see some flare in the upper left corner. You can also see some vignetting, which I didn't really notice at the time.
                <br><br>
                So I decide to switch to a different target, something higher up in the sky. Even though it's kinda small for a 300mm lens, I picked the <a href="https://en.wikipedia.org/wiki/Dumbbell_Nebula">Dumbbell Nebula</a>. Apparently it's quite easy for beginners, so even if it did turn out to be tiny, why the hell not?
                <br><br>
                1 hour and 43 minutes of integration at 64000 iso, 300mm
            </p>
            <img class="demoImage" width=1000 src="https://i.imgur.com/viA2K7n.jpg">
            <p>
                Super cool, even though it's faint and a little blurry. At this point, I kinda clued into the fact that a having wide open aperture was causing the vignette effect and also contributing to the lack of sharpness in the final image.
                <br><br>
                So last night I fixed my lens on the <a href="https://en.wikipedia.org/wiki/Veil_Nebula">Veil Nebula<a>. My reasoning was that first off, It's huge. Secondly, it has a magnitude of 7.00 which is like 0.40 brighter than the dumbbell nebula. What I didn't consider was that surface brightness is 18.15 meg/arcmin. This basically means that the object itself is bright, but the brightness per surface area is not. Another lesson learned: magnitude is important, but so is the object size. So, that paired with the fact that I was using a smaller aperture lead to disappointing results.
            </p>
            <img class="demoImage" width=1000 src="https://i.imgur.com/nyfKa3Y.jpg">
            <p>
                There were some positives though. The smaller aperture did eliminate the vignette and I also found that the stars (even though they're trailing a little) are sharper.
                <br><br>
                It is really faint but if you look hard enough you can kinda start to see the nebula showing up, which is kinda neat!
            </p>
            <img class="demoImage" width=1000 src="https://i.imgur.com/SZuLxtR.jpg">
            <p>
                Not going to lie, manually tracking stars for hours on end and waiting hours for images to be processed/stacked only to get a crappy picture is really discouraging. Even though I don't have anything amazing to show for my efforts, I'm determined to capture something incredible.
                <br><br>
                Here's a list of the software/sites that I found useful.
                <ul class="list">
                    <li>Stacking: <a href="http://deepskystacker.free.fr/english/index.html">Deep Space Stacker</a></li>
                    <li>Stretching and processing: Photoshop cc 2019</li>
                    <li>Locating DSOs and getting info: <a href="https://stellarium.org/">Stellarium</a></li>
                    <li>Observation conditions <a href="https://www.cleardarksky.com">Clear Dark Sky</a></li>
                </ul>
            </p>
        </div>
        <div class="article">
            <h1>A continuation of my exploration of noise.</h1>
            <h4 class="date">June 11, 2020</h4>
            <hr>
            <p>
                So after creating the bodies of water, I needed a way to give them "life". After some googling, I came across this <a href="https://twitter.com/TheRujiK/status/1208035937671884800">twitter post</a>
                and thought it looked pretty badass. So for the most part, that's what I was using for a reference.
                <br><br>
                First things first was creating a way to calculate simplex noise. I'm lazy naturally, so instead of reinventing the wheel
                I found this <a href="https://gist.github.com/patriciogonzalezvivo/670c22f3966e662d2f83">awesome resource</a> for noise functions.
            </p>
            <p>This was the noise code that was used to generate the follwing image.</p>
            <pre><code class="language-clike">vec3 permute(vec3 x) { return mod(((x*34.0)+1.0)*x, 289.0); }

float snoise(vec2 v){
    const vec4 C = vec4(0.211324865405187, 0.366025403784439, -0.577350269189626, 0.024390243902439);
    vec2 i  = floor(v + dot(v, C.yy) );
    vec2 x0 = v - i + dot(i, C.xx);
    vec2 i1;
  
    i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
  
    vec4 x12 = x0.xyxy + C.xxzz;
    x12.xy -= i1;
  
    i = mod(i, 289.0);
    vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))
    + i.x + vec3(0.0, i1.x, 1.0 ));
    vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);
    m = m*m ;
    m = m*m ;
    vec3 x = 2.0 * fract(p * C.www) - 1.0;
    vec3 h = abs(x) - 0.5;
    vec3 ox = floor(x + 0.5);
    vec3 a0 = x - ox;
    m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );
    vec3 g;
    g.x  = a0.x  * x0.x  + h.x  * x0.y;
    g.yz = a0.yz * x12.xz + h.yz * x12.yw;
  
    return 130.0 * dot(m, g);
}

float OctavePerlin(vec2 v, float persistence) {
    float total = 0.0;
    float frequency = 1.0;
    float amplitude = 1.0;
    float maxValue = 0.0;  // Used for normalizing result to 0.0 - 1.0
    for(int i = 0; i < 8; i++) {
        total += snoise(v * frequency) * amplitude;
        
        maxValue += amplitude;
        
        amplitude *= persistence;
        frequency *= 2.0;
    }
    return total/maxValue;
}</code>
            </pre>
            <p>And here are the results, which look as expected</p>
            <img class="demoImage" width="500" height="500" src="images/noise1.PNG">
            <p>
                Now it's time to start messing around and trying to get this to look like water!
                I first started by reducing the number of octaves to 1, which gives it a more "blobby" look.
            </p>
            <img class="demoImage" width="500" height="500" src="images/noise2.png">
            <p>To give the illusion of movement, we need this bad boy to start scrolling, which is quite simple.</p>
            <pre><code class="language-clike">float time = u_time;
vec3 color = vec3(0.0);
vec2 vel = vec2(-1.,0.0) * time;

color = vec3(OctavePerlin(pos + vel, 0.5));</code>
            </pre>
            <img class="demoImage" width="500" height="500" src="images/water1.gif">
            <p>I then stretch the texture using a scale matrix.</p>
            <img class="demoImage" width="500" height="500" src="images/water2.gif">
            <p>It reall starts to come to life when using two scrolling textures.</p>

            <pre><code class="language-clike">float time = u_time;
vec3 color = vec3(0.0);

vec2 vel1 = vec2(-1.,0.0)*time;
vec2 vel2 = vec2(1.0,0.0)*time;

color = vec3(OctavePerlin(pos + vel1, 0.5));
color += vec3(OctavePerlin(pos + vel2, 0.5));</code>
            </pre>
            <img class="demoImage" width="500" height="500" src="images/water3.gif">
            <p>
                I really like the look and movement at this point. The only problem is that this doesn't look like water.
                The idea is that this black and white scrolling texture is a height map, so all we need to do is find a way to convert a height map into a normal map.
                This is accomplished by using image processing technique called the <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel operator</a>. So by sampling the surrounding pixels in the image, we can calculate the approximate direction of the normal.
                <br>
                This is the result
                <br>
            </p>
            <img class="demoImage" width="500" height="500" src="images/water4.gif">
            <p>and the final product with a simple phong shader</p>
            <img class="demoImage" width="500" height="500" src="images/water5.gif">
            <p>I'm pretty happy with these results. This will really come to life with reflections (SSR) which is the next thing I'm going to tackle.</p>
        </div>

        <div class="article">
            <h1>Noise and Terrain Generation</h1>
            <h4 class="date">June 04, 2020</h4>
            <hr>
            <p>
                I've started to work on my game project once again and wanted to start creating interesting and natural enviroments.
                In the past I've spent a ludicris amount of time mundanly placing objects around a map. Not only is this boring, but it's painful. So this time around I decided to create procedural maps with automatic foliage placement by using a combination of possian disk distribution and perlin noise.
                <br>
                I was inspired by this <a href="https://www.youtube.com/watch?v=NfizT369g60">video</a> on procedural environments for FarCry 5.
                <br>
                This post mainly deals with generating the main terrain height map and isolating the lowest (or darkest) regions into
                a wavefront obj file.
            </p>

            <p>I started by generating a simple image of simplex noise.</p>
            <pre><code class="language-python">import noise
def genTerrain(size, scale):
    dimensions = (size, size)
    octaves = 5
    persistence = 0.5
    lacunarity = 2.0
    offset = 0.5

    noiseMap = np.zeros(dimensions)

    for i in range(dimensions[0]):
        for j in range(dimensions[1]):
            noiseMap[i][j] = (noise.snoise2(
                                i/scale,
                                j/scale,
                                octaves=octaves,
                                persistence=persistence,
                                lacunarity=lacunarity,
                                repeatx=size,
                                repeaty=size,
                                base=0) + offset)
    return noiseMap

mapTerrain = genTerrain(1024, 500)</code>
</pre>
            <img class="demoImage" width="500" height="500" src="https://i.imgur.com/zOPyT3f.png">
            <p>Next, I simplified the image by limiting the number of levels used. Here I'm using 8 levels of grey.</p>
            <pre><code class="language-python">def colorNoiseMap(noiseMap, numberOfColors):
    noiseMapColored = np.zeros(noiseMap.shape+(3,), dtype=np.uint8)
    colorDict = getColorDict(numberOfColors)

    buckets = [round(x, 5) for x in drange(0.0, 1.0, 1.0 / numberOfColors)]

    for i in range(noiseMap.shape[0]):
        for j in range(noiseMap.shape[1]):
            bucket = take_closest(buckets, noiseMap[i][j])
            noiseMapColored[i][j] = colorDict.get(bucket)

    return noiseMapColored</code>
            </pre>
            <img class="demoImage" width="500" height="500" src="https://i.imgur.com/XbH1KG2.png">
            <p>
                I wanted the darker regions in the image to be the bodies of water and to have the lighter regions
                populated with trees, grass, and other foliage.
                Using cv2, I found the contours of each body of water and store that as a list of points.
            </p>
            <img class="demoImage" width="500" height="500" src="https://i.imgur.com/m2nMyn3.png">
            <p>
                Each of these contours are then triangulated (thanks to <a href="https://gis.stackexchange.com/questions/316697/delaunay-triangulation-algorithm-in-shapely-producing-erratic-result">nickves</a>)
                and stored into a wavefront obj file.
            </p>
            <pre><code class="language-python">def createWavefrontOBJ(lakes):
    vertices = []
    polygons = []

    for lake in lakes:
        lakePoly = Polygon(lake)
        if (lakePoly.is_valid):
        lakeTriangles = triangulatePoints(lakePoly)
        for t in lakeTriangles:
            v = list(t.exterior.coords)

    v1 = (v[0][0], v[0][1], 0.0)
    v2 = (v[1][0], v[1][1], 0.0)
    v3 = (v[2][0], v[2][1], 0.0)

    vertices.append(v1)
    vertices.append(v2)
    vertices.append(v3)

    size = len(vertices)

    polygons.append((size-2,size-1,size))

    f = open("water.obj", "w")

    for v in vertices:
        f.write("v "+str(v[0])+" "+str(v[1])+" "+str(v[2])+"\n")

    f.write("\nvn 0.0 1.0 0.0\n\n")

    for p in polygons:
        f.write("f "+str(p[0])+"//1 "+str(p[1])+"//1 "+str(p[2])+"//1\n")

    f.close()</code>
            </pre>
            <img class="demoImage" width="500" height="500" src="https://i.imgur.com/lRkRei3.png">
            <p>
                The code is not the nicest, but I did have a lot of fun writing this up. The only
                thing that was a pain was getting all the libraries installed correctly.
                <br>
                The next thing to do is create a shader for the water and create a way to place objects using poisson disk distribution, which shouldn't be difficult.
            </p>
        </div>
    </div>
</body>
</html>
